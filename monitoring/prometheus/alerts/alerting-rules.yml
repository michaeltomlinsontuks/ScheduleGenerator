# Alerting Rules for UP Schedule Generator
# These rules define when alerts should be triggered based on metrics

groups:
  # =============================================================================
  # Error Rate Alerts
  # =============================================================================
  - name: error_rate_alerts
    interval: 30s
    rules:
      # Alert when error rate exceeds 5%
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_request_duration_seconds_count{status=~"5.."}[5m]))
            /
            sum(rate(http_request_duration_seconds_count[5m]))
          ) > 0.05
        for: 2m
        labels:
          severity: critical
          component: backend
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"
          dashboard: "http://grafana:3000/d/system-overview"

      # Alert when job failure rate is high
      - alert: HighJobFailureRate
        expr: |
          (
            sum(rate(pdf_jobs_total{status="failed"}[10m]))
            /
            sum(rate(pdf_jobs_total[10m]))
          ) > 0.10
        for: 5m
        labels:
          severity: warning
          component: pdf-worker
        annotations:
          summary: "High PDF job failure rate"
          description: "Job failure rate is {{ $value | humanizePercentage }} (threshold: 10%)"
          dashboard: "http://grafana:3000/d/job-processing"

  # =============================================================================
  # Response Time Alerts
  # =============================================================================
  - name: response_time_alerts
    interval: 30s
    rules:
      # Alert when p95 response time exceeds 10 seconds
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le, route)
          ) > 10
        for: 3m
        labels:
          severity: critical
          component: backend
        annotations:
          summary: "High response time on {{ $labels.route }}"
          description: "P95 response time is {{ $value | humanizeDuration }} (threshold: 10s)"
          dashboard: "http://grafana:3000/d/system-overview"

      # Alert when PDF processing time is excessive
      - alert: SlowPDFProcessing
        expr: |
          histogram_quantile(0.95,
            sum(rate(pdf_processing_duration_seconds_bucket{status="completed"}[10m])) by (le, type)
          ) > 120
        for: 5m
        labels:
          severity: warning
          component: pdf-worker
        annotations:
          summary: "Slow PDF processing for {{ $labels.type }}"
          description: "P95 processing time is {{ $value | humanizeDuration }} (threshold: 120s)"
          dashboard: "http://grafana:3000/d/job-processing"

  # =============================================================================
  # Resource Utilization Alerts
  # =============================================================================
  - name: resource_alerts
    interval: 30s
    rules:
      # Alert when database connection pool utilization exceeds 80%
      - alert: HighDatabaseConnectionUsage
        expr: |
          (
            database_connections{state="active"}
            /
            database_connections{state="total"}
          ) > 0.80
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "High database connection pool utilization"
          description: "Connection pool is {{ $value | humanizePercentage }} utilized (threshold: 80%)"
          dashboard: "http://grafana:3000/d/resource-utilization"

      # Alert when memory usage is high (using Node.js heap metrics)
      - alert: HighMemoryUsage
        expr: |
          (
            nodejs_heap_size_used_bytes
            /
            nodejs_heap_size_total_bytes
          ) > 0.80
        for: 5m
        labels:
          severity: warning
          component: backend
        annotations:
          summary: "High memory usage in backend"
          description: "Heap usage is {{ $value | humanizePercentage }} (threshold: 80%)"
          dashboard: "http://grafana:3000/d/resource-utilization"

      # Alert when event loop lag is high
      - alert: HighEventLoopLag
        expr: nodejs_eventloop_lag_seconds > 1
        for: 3m
        labels:
          severity: warning
          component: backend
        annotations:
          summary: "High event loop lag detected"
          description: "Event loop lag is {{ $value | humanizeDuration }} (threshold: 1s)"
          dashboard: "http://grafana:3000/d/resource-utilization"

  # =============================================================================
  # Queue Depth Alerts
  # =============================================================================
  - name: queue_alerts
    interval: 30s
    rules:
      # Alert when queue depth exceeds 500 jobs
      - alert: HighQueueDepth
        expr: queue_jobs_waiting{queue="pdf-processing"} > 500
        for: 5m
        labels:
          severity: critical
          component: queue
        annotations:
          summary: "High queue depth detected"
          description: "Queue has {{ $value }} waiting jobs (threshold: 500)"
          dashboard: "http://grafana:3000/d/job-processing"

      # Warning when queue depth exceeds 200 jobs
      - alert: ElevatedQueueDepth
        expr: queue_jobs_waiting{queue="pdf-processing"} > 200
        for: 10m
        labels:
          severity: warning
          component: queue
        annotations:
          summary: "Elevated queue depth"
          description: "Queue has {{ $value }} waiting jobs (threshold: 200)"
          dashboard: "http://grafana:3000/d/job-processing"

  # =============================================================================
  # Service Health Alerts
  # =============================================================================
  - name: service_health_alerts
    interval: 30s
    rules:
      # Alert when backend is down
      - alert: BackendDown
        expr: up{job="backend"} == 0
        for: 1m
        labels:
          severity: critical
          component: backend
        annotations:
          summary: "Backend service is down"
          description: "Backend has been unreachable for more than 1 minute"

      # Alert when Prometheus cannot scrape metrics
      - alert: MetricsScrapeFailure
        expr: up{job="backend"} == 0
        for: 3m
        labels:
          severity: warning
          component: monitoring
        annotations:
          summary: "Failed to scrape metrics from {{ $labels.job }}"
          description: "Prometheus cannot scrape metrics from {{ $labels.job }}"

  # =============================================================================
  # Job Processing Alerts
  # =============================================================================
  - name: job_processing_alerts
    interval: 30s
    rules:
      # Alert when no jobs are being processed (possible worker failure)
      - alert: NoJobsProcessing
        expr: |
          rate(pdf_jobs_total[10m]) == 0
          and
          queue_jobs_waiting{queue="pdf-processing"} > 10
        for: 10m
        labels:
          severity: critical
          component: pdf-worker
        annotations:
          summary: "PDF workers may be stuck"
          description: "No jobs processed in 10 minutes despite {{ $value }} jobs waiting"
          dashboard: "http://grafana:3000/d/job-processing"

      # Alert when job processing rate drops significantly
      - alert: LowJobProcessingRate
        expr: |
          rate(pdf_jobs_total[5m]) < 0.1
          and
          queue_jobs_waiting{queue="pdf-processing"} > 50
        for: 10m
        labels:
          severity: warning
          component: pdf-worker
        annotations:
          summary: "Low job processing rate"
          description: "Processing rate is {{ $value }} jobs/sec with {{ $labels.queue_jobs_waiting }} jobs waiting"
          dashboard: "http://grafana:3000/d/job-processing"
